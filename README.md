This collection of scripts prepares the input files used in LLR simulations with heatbath updating.
I am currently rewriting them for simplicity. 

### Introduction

When using the replica exchange (aka "parallel tempering" aka "umbrella method") in HiRep, a specific structure of input files and directories is required. In contrast to standard HiRep simulations, the usual input file has to be split in two parts:

1. The input file that contains the global parameters: Lattice size, parallelization and logging. 

2. The input file that contains all other information, including information that changes across replicas.

The required structure within a given directory for a run with two replicas has the form:

```
├── input_file
├── Rep_0
│   └── input_file
└── Rep_1
    └── input_file
```

The name `input_file` is the default choice in the current versions of HiRep. An alternative input file name can be provided by executing the binary `llr_hb` with the flag `-i`. Note that the input file for the replicas and the input file for the global parameters must have the same name. This script will use this.

### Strategies used on the cluster

In general, we need more than one LLR calculation with the same external parameter. In addition, we cannot perform an entire run in a single job on the cluster due to wall time constraints.

The scrips generated by this code deals with both aspects. We split it into four stages. 

1. Thermalization
2. Newton-Raphson (NR) updates
3. Robbins-Monro (RM) updates
4. Fixed-$a_n$ measurements

For each of this stage we prepare separate input files. The template for them can be found in `input/templates/base`. The prepared input files are then stored in the output directory.

For every stage we provide dedicated bash/slurm files than can be used to run them for a single repeat. They are called

1. `sp4_llr_start.sh`
2. `sp4_llr_start_cont.sh`
3. `sp4_llr_cont.sh`
4. `sp4_llr_fxa.sh`

Most of the time, we want to run/submit all four stages at ones an repeat this many times. This is done via the script `setup_llr_repeat.sh`. In this script the number of repeats can be chosen.

Steps 2. and 3. can be repeated on slurm system by a specified number of times. The script `setup_llr_repeat.sh` does this via the use of slurm array jobs. 

The script `setup_llr_repeat.sh` creates a copy of the prepared input files from the `output/$RUNNAME/base` directory and places them in a separate directory for each repeat. These have then the following structure:

```
├── info.csv
├── input_file_cont
├── input_file_fxa
├── input_file_start
├── input_file_start_cont
├── Rep_0
│   ├── input_file_cont
│   ├── input_file_fxa
│   ├── input_file_start
│   └── input_file_start_cont
├── Rep_1
│   ├── input_file_cont
│   ├── input_file_fxa
│   ├── input_file_start
│   └── input_file_start_cont
```
The file `info.csv` is a copy of the input for the initial script. When running the LLR the output files will be placed in the individual replica directories as `Rep_XX/out_0`. Furthermore, the last configuration and the RNG state will be saved here.

### Usage

The input parameters are collected in a .csv file. A test file is provided in `input/local_tests.csv` This file is then used by the script `setup_run.py` which generates the structure as described above. In addition, it contains bash scripts that allow it to run either locally or on any of the provided clusters via slurm. Changes can be made in the templates in `templates/csd3` and `templates/dial3`

Currently, our project number (dp208) and my email (fabian.zierler@swansea.ac.uk) is hard-coded. 

### Parameters

- `Lt`: temporal lattice extent
- `Ls`: spatial lattice extent
- `n_replicas`: Number of replicas
- `n_repeats`: Number of initial repeats (unused)
- `N_NR`: Number of NR steps in the slurm array job
- `N_RM`: Number of RM steps in the slurm array job
- `umin`: Minimal value of the plaquette in this run
- `umax`: Maximal value of the plaquette in this run
- `N_th`: Number of initial thermalization steps
- `N_meas`: Number of measurement steps in an NR/RM update
- `Nfs_swaps`: Frequency of swaps in fixed-$a_n$ calculation
- `Nfa_meas` : Number of measurement steps in fixed-$a_n$ calculation (unused)
- `PX`: Domain decomposition in the spatial $x$-direction. Set to `1` for no domain decomposition.
- `an_file`: Input file for the initial $a_n$ values. Should be a csv with the first column denoting $a_n$ or $\beta$ and the second one the corresponding average plaquette. See `input/data/` for examples obtained using importance sampling. An interpolation between the provided data points is used.
- `machine`: Choose where to run this. Options are "local", "csd3" and "dial3"
- `cores_per_node`: Specify the number of cores per node available on the chosen machine.
- `N_NR_per_step`: The number of actual NR steps performed in every slurm array step
- `N_RM_per_step`: The number of actual RM steps performed in every slurm array step


### Testing 

The script `run_test.sh` can be used to download HiRep, generate sample input files and run the LLR. By default, this uses the input templates in `input/local/` for a small lattice and sets all RNG seeds to one.

It assumes that `gcc`, `openmpi`, `diff` and `meld` are available. 

This is repeated a second time and checked for differences using `diff` and `meld`. For development, it is useful to then keep one copy around and use it to identify regressions.  

### More information

See the document `Notes_on_HiRep_with_replica_exchanges.md`

### TODO:

1. Update the RNG seed for the thermalization for every repeat.
2. Remove hard-coded project number and email address.
3. Remove hard-coded fixed-$a_n$ parameters
4. Use input parameters marked as "unused" above